{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfOXU_5dWdL3"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchtext.legacy.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CI9exT8iNu5"
      },
      "source": [
        "Наиболее распространенными моделями последовательностей (seq2seq) являются модели кодер-декодер, которые (обычно) используют рекуррентную нейронную сеть (RNN) для кодирования исходного (входного) предложения в один вектор.  \n",
        "Вы можете думать о векторе контекста как об абстрактном представлении всего входного предложения. Этот вектор затем декодируется декодером, который учится выводить  предложение, генерируя его по одному слову за раз.\n",
        "\n",
        "$h_t = \\text{Encoder}(x_t, h_{t-1})$\n",
        "\n",
        "У нас есть последовательность $X = \\{x_1, x_2, ..., x_T\\}$, где $x_1 = \\text{<sos>;}, x_2 = \\text{the}$, и так далее. Начальное состояние, $h_0$,  может быть инициализировано вектором из нулей или обучаемым.\n",
        "\n",
        "\n",
        "Как только последнее слово, $x_T$, был подан на Encoder, мы  используем  информацию в  последнем скрытом состоянии, $h_T$, в зависимости от контекста вектор, т. е. $h_T $ это векторное представление всего исходного предложения.\n",
        "\n",
        "После получения вектора всего предложения мы можем декодировать предложение уже на новом языке. На каждом шаге декодирования мы подаем правильное слово $y_t$,  дополняем это информацией о скрытом состоянии $s_{t-1}$, где  $s_t = \\text{DecoderRNN}(y_t, s_{t-1})$\n",
        "\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/f6DQb.png)\n",
        "\n",
        "\n",
        "Мы всегда используем $<sos>$ для первого входа в декодер, $y_1$, но для последующих входов, $y_{\\text{from }t; 1}$, мы иногда будем использовать фактическое, основное истинное следующее слово в последовательности, $y_t$, а иногда использовать слово, предсказанное нашим декодером, $\\hat{y}_{t-1}$. Использование настоящих токенов в декодере называется Teacher Forcing [можно тут посмотреть](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)\n",
        "\n",
        "Мы будем  использовать TorchText и spaCy( как токенизатор) , чтобы помочь вам выполнить всю необходимую предварительную обработку быстрее чем мы делали раньше. В данной работе вам предлагается написать модель Seq2Seq и обучить ее на Multi30k. В данном задание мы будем подавать на вход перевернутые предложения, так как авторы seq2seq считали, что это улучшает качество перевода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW9nwc2niWiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfacb8f6-fc91-494a-bfc6-b3782668761b"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yONlhzjmiZv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cde58c6-0fd7-4ae3-e714-d967ca263079"
      },
      "source": [
        "! python -m spacy download en\n",
        "! python -m spacy download de\n",
        "\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=d2e461d95fce99dc83756d554464f1d4d6a308864fed3abcd4068152c101c7d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vv70m2ak/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196S34mIiffl"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    # токенизируем немецкий текст в список токенов и перевернем\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # токенизиурем английский текст в список токенов\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# немецкий язык является полем SRC, а английский в поле TRG\n",
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3LSvTtAjBLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1bd553-e639-4d17-d3d3-b9e7b5b37ec1"
      },
      "source": [
        "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:01<00:00, 702kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46.3k/46.3k [00:00<00:00, 227kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.2k/66.2k [00:00<00:00, 218kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnGvgAJMjI9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab34d341-3182-4584-8de9-ca79925630b7"
      },
      "source": [
        "labels = ['train', 'validation', 'test']\n",
        "dataloaders = [train_data, valid_data, test_data]\n",
        "\n",
        "for d, l in zip(dataloaders, labels):\n",
        "    print(f\"Кол-во предложений {l} : {len(d.examples)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кол-во предложений train : 29000\n",
            "Кол-во предложений validation : 1014\n",
            "Кол-во предложений test : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX5dUSzzkPKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6f9699-1109-43ce-d2d0-3e16bf193f73"
      },
      "source": [
        "train_data.examples[1].src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " 'antriebsradsystem',\n",
              " 'ein',\n",
              " 'bedienen',\n",
              " 'schutzhelmen',\n",
              " 'mit',\n",
              " 'männer',\n",
              " 'mehrere']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLZvFiBdkUtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef26ea18-cb6f-4162-dad7-1aaa7380ab4b"
      },
      "source": [
        "train_data.examples[1].trg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['several',\n",
              " 'men',\n",
              " 'in',\n",
              " 'hard',\n",
              " 'hats',\n",
              " 'are',\n",
              " 'operating',\n",
              " 'a',\n",
              " 'giant',\n",
              " 'pulley',\n",
              " 'system',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPbSfnRwkkVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4ac556-b83d-4674-f5d7-77025de2923c"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "print(\"Кол-во слов исходного\", len(SRC.vocab))\n",
        "print(\"Кол-во target\", len(TRG.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кол-во слов исходного 7855\n",
            "Кол-во target 5893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SqD9n8jkxeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f910582-7322-478b-e929-549515fadb81"
      },
      "source": [
        "SRC.process([\"wie geht es dir\",])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2],\n",
              "        [   0],\n",
              "        [3833],\n",
              "        [5886],\n",
              "        [ 664],\n",
              "        [   0],\n",
              "        [5886],\n",
              "        [   0],\n",
              "        [   0],\n",
              "        [ 664],\n",
              "        [5886],\n",
              "        [7098],\n",
              "        [ 664],\n",
              "        [   0],\n",
              "        [3833],\n",
              "        [7011],\n",
              "        [   3]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYEor-1dk9an"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "Напишем для начала простой Encoder, который реализует следующий функционал:\n",
        "\n",
        "$ (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
        "\n",
        "В  методе forward мы передаем исходное предложение $X$, которое преобразуется в embeddings, к которым применяется dropout . Эти вектора затем передаются в RNN. Когда мы передадим всю последовательность RNN, он автоматически выполнит для нас рекуррентный расчет скрытых состояний по всей последовательности.\n",
        "\n",
        "Вы можете заметить, что мы не передаем начальное скрытое или состояние ячейки в RNN. Это происходит потому, что, как отмечено в документации, если никакое скрытое состояние/ячейки не передается RNN, он автоматически создаст начальное скрытое состояние/ячейки как тензор всех нулей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilTmLDIUlP8B"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        \"\"\"\n",
        "        input_dim - это размер / размерность one hot векторов, которые будут вводиться в encoder. Равно размеру входного (исходного) словаря.\n",
        "        emb_dim - это размерность слоя embedding-a. Этот слой преобразует one-hot векторы в сжатие векторы с размерами emb_dim.\n",
        "        hid_dim - это размерность скрытого и cell состояний.\n",
        "        n_layers - это количество слоев в RNN\n",
        "        dropout - процент dropout\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        # src - предложения размера (src_len x batch_size)\n",
        "        # embedded = <TODO> \n",
        "        embedded = self.embedding(src) # (src_len x batch_size x embd_dim)\n",
        "        embedded = self.dropout(embedded) # dropout эмбеддингов\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z272mTu9mL1X"
      },
      "source": [
        "## Decoder\n",
        "Похожий на Encoder, но со слоем проекцией, который переводит из hidden_dim в output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT3ZJfspmLHA"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n",
        "        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers) # размер (lstm embd, hid, layers, dropout)\n",
        "        self.out = nn.Linear(self.hid_dim, self.output_dim) # проекция hid_dim x output_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input_, hidden, cell):\n",
        "        input_ = input_.unsqueeze(0) # 1 размер батча\n",
        "        # (1 x batch_size x emb_dim)\n",
        "        # эмбеддинг по input and dropout \n",
        "        embedded = self.embedding(input_) \n",
        "        embedded = self.dropout(embedded)\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        # batch_size x output_dim\n",
        "        prediction = self.out(output.squeeze(0)) # проэкция из рнн на выходное\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3CnjWWym4bi"
      },
      "source": [
        "## Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFaUY32pmC6d"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self._init_weights()  \n",
        "    \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        '''\n",
        "        src - входное предл (src_len x batch_size)\n",
        "        trg - выходное предл\n",
        "        teacher_forcing_ration - 0.5 вероятность получить реальный токен вместо предложенного\n",
        "        '''\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # сохраняем выходы декодера \n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # последнее скрытое энкодера используем как первый хидден декодера\n",
        "        hidden, cell = self.encoder(src) #\n",
        "        \n",
        "        # первый инпут декодера - токен <sos> \n",
        "        input = trg[0, :]\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell) # передаем состояние и вход через декодер\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            input = (trg[t] if teacher_force else top1)\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        p = 0.08\n",
        "        for name, param in self.named_parameters():\n",
        "            nn.init.uniform_(param.data, -p, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAovAs76oFbW"
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "output_dim = len(TRG.vocab)\n",
        "src_embd_dim =  tgt_embd_dim = 128\n",
        "hidden_dim = 512\n",
        "num_layers =  4\n",
        "dropout_prob = 0.2\n",
        "\n",
        "batch_size = 256\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                  batch_size = batch_size, device = device)\n",
        "train_iterator, valid_iterator, test_iterator = iterators\n",
        "\n",
        "enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
        "dec = Decoder(output_dim, tgt_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V8TDdF3oMBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f592d3-fecc-4d88-a572-b0716adcbeda"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 128)\n",
              "    (rnn): LSTM(128, 512, num_layers=4)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 128)\n",
              "    (rnn): LSTM(128, 512, num_layers=4)\n",
              "    (out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6soBsY13oOt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5f5450-7f59-454b-9c71-8fe249af430c"
      },
      "source": [
        "test_batch = next(iter(test_iterator))\n",
        "test_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.legacy.data.batch.Batch of size 256 from MULTI30K]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 12x256 (GPU 0)]\n",
              "\t[.trg]:[torch.cuda.LongTensor of size 14x256 (GPU 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyZXWbbDoRbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fccae2e-2251-49f4-b0f6-e47d1c3f74bd"
      },
      "source": [
        "test_batch.src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
              "        [   4,    4,    4,  ...,    4,  714,    4],\n",
              "        [   0,   88,  201,  ...,  669,   12, 1643],\n",
              "        ...,\n",
              "        [  16,   32,   13,  ...,    1,    1,    1],\n",
              "        [   8,    5,    5,  ...,    1,    1,    1],\n",
              "        [   3,    3,    3,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeF8tqVkodXI"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc6UGXSFoiwj"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) \n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeVnlZmSooL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0e9de4-976d-438c-9fd8-4f0f7b45992f"
      },
      "source": [
        "max_epochs = 20\n",
        "CLIP = 1\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "        \n",
        "    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
        "    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n",
        "    \n",
        "    \n",
        "    if valid_loss < best_valid_loss: # сохраняем модель, если преплексия улучшилась\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/model.pt')\n",
        "    \n",
        "    print(f'Эпоха: {epoch} \\n Train Loss {train_loss}  Val loss {valid_loss}')\n",
        "    print(f'Train Perplexity {np.exp(train_loss)}  Val Perplexity {np.exp(valid_loss)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 0 \n",
            " Train Loss 5.0364  Val loss 4.93117\n",
            "Train Perplexity 153.9149227374165  Val Perplexity 138.5415111202048\n",
            "Эпоха: 1 \n",
            " Train Loss 4.91984  Val loss 4.80724\n",
            "Train Perplexity 136.98069452189904  Val Perplexity 122.39334528333556\n",
            "Эпоха: 2 \n",
            " Train Loss 4.86937  Val loss 4.77773\n",
            "Train Perplexity 130.2388405755698  Val Perplexity 118.83428980455093\n",
            "Эпоха: 3 \n",
            " Train Loss 4.84564  Val loss 4.76274\n",
            "Train Perplexity 127.18465413051922  Val Perplexity 117.0662483773441\n",
            "Эпоха: 4 \n",
            " Train Loss 4.82351  Val loss 4.74754\n",
            "Train Perplexity 124.40097277385152  Val Perplexity 115.30029663557252\n",
            "Эпоха: 5 \n",
            " Train Loss 4.80085  Val loss 4.7669\n",
            "Train Perplexity 121.61374528170383  Val Perplexity 117.55425832751496\n",
            "Эпоха: 6 \n",
            " Train Loss 4.76807  Val loss 4.71777\n",
            "Train Perplexity 117.69187730115885  Val Perplexity 111.91839615791112\n",
            "Эпоха: 7 \n",
            " Train Loss 4.70703  Val loss 4.73805\n",
            "Train Perplexity 110.72282429624559  Val Perplexity 114.21127241355805\n",
            "Эпоха: 8 \n",
            " Train Loss 4.65731  Val loss 4.69961\n",
            "Train Perplexity 105.35230294981248  Val Perplexity 109.90430141526274\n",
            "Эпоха: 9 \n",
            " Train Loss 4.61288  Val loss 4.66944\n",
            "Train Perplexity 100.77396111555446  Val Perplexity 106.63800842377212\n",
            "Эпоха: 10 \n",
            " Train Loss 4.54511  Val loss 4.69642\n",
            "Train Perplexity 94.17078542609134  Val Perplexity 109.55426529818827\n",
            "Эпоха: 11 \n",
            " Train Loss 4.48281  Val loss 4.63241\n",
            "Train Perplexity 88.48296078729126  Val Perplexity 102.76142095026894\n",
            "Эпоха: 12 \n",
            " Train Loss 4.41408  Val loss 4.61555\n",
            "Train Perplexity 82.60580859270159  Val Perplexity 101.04338711538354\n",
            "Эпоха: 13 \n",
            " Train Loss 4.37569  Val loss 4.62457\n",
            "Train Perplexity 79.49467195655637  Val Perplexity 101.95892131907901\n",
            "Эпоха: 14 \n",
            " Train Loss 4.31144  Val loss 4.59534\n",
            "Train Perplexity 74.54776046137819  Val Perplexity 99.02179723591686\n",
            "Эпоха: 15 \n",
            " Train Loss 4.27532  Val loss 4.57087\n",
            "Train Perplexity 71.90314463489385  Val Perplexity 96.62813969606346\n",
            "Эпоха: 16 \n",
            " Train Loss 4.22802  Val loss 4.55574\n",
            "Train Perplexity 68.58130666480305  Val Perplexity 95.17716028110637\n",
            "Эпоха: 17 \n",
            " Train Loss 4.20034  Val loss 4.56952\n",
            "Train Perplexity 66.70900824838586  Val Perplexity 96.49777972025582\n",
            "Эпоха: 18 \n",
            " Train Loss 4.16492  Val loss 4.56388\n",
            "Train Perplexity 64.38753123491803  Val Perplexity 95.9550641392019\n",
            "Эпоха: 19 \n",
            " Train Loss 4.13448  Val loss 4.53987\n",
            "Train Perplexity 62.45710493023736  Val Perplexity 93.67862110711135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN5s7qTcpFlS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "254cc8e3-ede6-47c8-e599-a7091d08d2d9"
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss} Test PPL: {np.exp(test_loss)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.603098034858704 Test PPL: 99.79299942936277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUbOYy1LpNdF"
      },
      "source": [
        "def translate(sentence):\n",
        "    sent_vec = SRC.process([sentence]).to(device)\n",
        "    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\n",
        "    input += SRC.vocab.stoi['<sos>']\n",
        "    output = model(sent_vec, input, 0)\n",
        "    for t in output:\n",
        "        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\n",
        "            print(TRG.vocab.itos[t[0].max(0)[1]], end=' ')\n",
        "        else:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbgWhsDUpO-G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9d7c2ab-2bbe-4ca7-ca2b-ff5705374947"
      },
      "source": [
        "translate(\"wie geht es dir\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gyro a group of a a a a a a "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzmd4dugwgj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}